from flask import Flask, request, jsonify, render_template
import os
import json
import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertForSequenceClassification
from sentence_transformers import SentenceTransformer, util
import openai

app = Flask(__name__)

# ğŸ” OpenAI API í‚¤
openai.api_key = os.getenv("OPENAI_API_KEY")

# âœ… ë¦¬ìŠ¤í¬ ë¶„ì„ìš© KoBERT ëª¨ë¸ ì´ˆê¸°í™”
MODEL_NAME = "5wqs/kobert-risk-final"
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
risk_model = BertForSequenceClassification.from_pretrained(MODEL_NAME)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
risk_model.to(device)
risk_model.eval()

# âœ… SBERT ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
embedder = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")

# âœ… í…œí”Œë¦¿ ë¡œë”© (id, title, file, snippet í¬í•¨)
TEMPLATE_PATH = "templates_index/templates_ids.json"
try:
    with open(TEMPLATE_PATH, "r", encoding="utf-8") as f:
        templates = json.load(f)
except Exception as e:
    print(f"[ì˜¤ë¥˜] í…œí”Œë¦¿ ë¡œë”© ì‹¤íŒ¨: {e}")
    templates = []

# âœ… í…œí”Œë¦¿ ì„ë² ë”© ì‚¬ì „ ê³„ì‚°
template_snippets = [t.get("snippet", t["title"]) for t in templates]
template_embeddings = embedder.encode(template_snippets, convert_to_tensor=True)

# âœ… í™ˆ
@app.route("/")
def index():
    return render_template("index.html")

# âœ… ê³„ì•½ì„œ ì´ˆì•ˆ ìƒì„±
@app.route("/generate_draft", methods=["POST"])
def generate_draft():
    data = request.get_json()
    party_a = data.get("party_a", "")
    party_b = data.get("party_b", "")
    subject = data.get("subject", "")
    date = data.get("date", "")

    prompt = f"""
ë„ˆëŠ” í•œêµ­ì–´ ê³„ì•½ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë²•ë¥  ë¹„ì„œì•¼.

ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³„ì•½ì„œë¥¼ ìì—°ìŠ¤ëŸ½ê³  ì¡°í•­ë³„ë¡œ ì‘ì„±í•´ì¤˜:

- ê³„ì•½ ë‹¹ì‚¬ì A: {party_a}
- ê³„ì•½ ë‹¹ì‚¬ì B: {party_b}
- ê³„ì•½ ëª©ì : {subject}
- íš¨ë ¥ ë°œìƒì¼: {date}

ì œ1ì¡° (ëª©ì ), ì œ2ì¡° (ê³„ì•½ ê¸°ê°„), ì œ3ì¡° (ê¶Œë¦¬ ë° ì˜ë¬´), ì œ4ì¡° (ë¹„ë°€ìœ ì§€), ì œ5ì¡° (ê³„ì•½ í•´ì§€), ì œ6ì¡° (ê¸°íƒ€ì‚¬í•­) í•­ëª© í¬í•¨.
ì‹¤ì œ ê³„ì•½ì„œì²˜ëŸ¼ ë²•ë¥  ë¬¸ì²´ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
    """

    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=1500,
        )
        draft = response.choices[0].message["content"]
        return jsonify({"draft": draft})
    except Exception as e:
        print("ì´ˆì•ˆ ìƒì„± ì˜¤ë¥˜:", e)
        return jsonify({"error": str(e)}), 500

# âœ… ë¦¬ìŠ¤í¬ ë¶„ì„
@app.route("/analyze_risk", methods=["POST"])
def analyze_risk():
    clause = request.json.get("clause", "")
    try:
        inputs = tokenizer(clause, return_tensors="pt", truncation=True, padding=True, max_length=512).to(device)
        with torch.no_grad():
            outputs = risk_model(**inputs)
            pred = torch.argmax(outputs.logits, dim=1).item()
        risk_label = "HighRisk" if pred == 1 else "LowRisk"
        return jsonify({"risk_label": risk_label})
    except Exception as e:
        print("ë¦¬ìŠ¤í¬ ë¶„ì„ ì˜¤ë¥˜:", e)
        return jsonify({"error": str(e)}), 500

# âœ… í…œí”Œë¦¿ ì¶”ì²œ (SBERT + Cosine Similarity)
@app.route("/recommend_templates", methods=["POST"])
def recommend_templates():
    clause = request.json.get("clause", "")
    try:
        clause_embedding = embedder.encode(clause, convert_to_tensor=True)
        similarities = util.cos_sim(clause_embedding, template_embeddings)[0]
        top_indices = torch.topk(similarities, k=3).indices.tolist()
        top_templates = [templates[i] for i in top_indices]
        return jsonify({"templates": top_templates})
    except Exception as e:
        print("í…œí”Œë¦¿ ì¶”ì²œ ì˜¤ë¥˜:", e)
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(debug=True)
