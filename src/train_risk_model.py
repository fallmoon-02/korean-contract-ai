import os
import pandas as pd
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments

def main():
    # 1) í•™ìŠµì— ì‚¬ìš©í•  ë¼ë²¨ë§ëœ CSV ê²½ë¡œ ì„¤ì •
    LABELED_CSV = os.path.join(os.getcwd(), "data", "clauses_labeled_full.csv")

    # 2) ë°ì´í„° ë¡œë“œ (pandas â†’ ğŸ¤— datasetsë¡œ ë³€í™˜)
    df = pd.read_csv(LABELED_CSV, encoding="utf-8-sig")
    print(f"[INFO] ë¼ë²¨ë§ëœ ë°ì´í„° ê°œìˆ˜: {len(df)}")

    # 3) ğŸ¤— datasets í˜•íƒœë¡œ ë³€í™˜
    ds = Dataset.from_pandas(df[["text", "label"]])

    # 4) KoBERT í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë“œ
    MODEL_NAME = "monologg/kobert"
    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

    # 5) ë°ì´í„°ì…‹ í† í°í™” í•¨ìˆ˜ ì •ì˜
    def tokenize_fn(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=128
        )

    tokenized_ds = ds.map(tokenize_fn, batched=True)
    tokenized_ds = tokenized_ds.rename_column("label", "labels")
    tokenized_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

    # 6) í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬ (80:20)
    split_ds = tokenized_ds.train_test_split(test_size=0.2, seed=2025)
    train_dataset = split_ds["train"]
    eval_dataset = split_ds["test"]

    # 7) Trainer ì¸ì ì„¤ì • (evaluation_strategy ì œê±°, do_eval + eval_steps ì‚¬ìš©)
    training_args = TrainingArguments(
        output_dir="kobert_risk_model",
        num_train_epochs=5,               # ì—í­ ìˆ˜ í•„ìš”ì‹œ ì¡°ì •
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        do_eval=True,                     # ê²€ì¦ ìˆ˜í–‰
        eval_steps=500,                   # 500 ìŠ¤í…ë§ˆë‹¤ ê²€ì¦
        save_total_limit=1,
        logging_steps=50,
    )

    # 8) Trainer ìƒì„±
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    # 9) ëª¨ë¸ í›ˆë ¨
    trainer.train()

    # 10) í›ˆë ¨ëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥
    model.save_pretrained("kobert_risk_model")
    tokenizer.save_pretrained("kobert_risk_model")
    print("[INFO] KoBERT ë¦¬ìŠ¤í¬ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ.")

if __name__ == "__main__":
    main()
